{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NityaVattam2002/Nitya_INFO5731_Fall2024/blob/main/Vattam_Nitya_Exercise_02_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "'''Research Question: How do search trends related to climate change differ across regions, and what factors influence these differences?\n",
        "\n",
        "Data Needed:\n",
        "Search terms related to climate change (\"global warming,\" \"carbon footprint\").\n",
        "Geographic location (country, city, or region).\n",
        "Date and time of searches.\n",
        "Search frequency (number of searches for each term).\n",
        "Potentially related keywords and topics.\n",
        "\n",
        "Amount of Data:\n",
        "A dataset of 1000 samples of search trends, with at least 100 samples from 10 different regions would be sufficient for initial analysis.\n",
        "\n",
        "Steps for Collecting Data:\n",
        "\n",
        "Choose a Scraping Method\n",
        "Define Search Keywords: Select a list of search terms related to climate change, such as \"climate change,\" \"global warming,\" \"renewable energy,\" etc.\n",
        "Select regions for comparison.\n",
        "Collect Data: Use the pytrends library to collect search trend data for the specified keywords and regions.\n",
        "Set a time frame (monthly or yearly trends) and gather data.\n",
        "Save Data:\n",
        "Store the collected data in a CSV file with the following columns:\n",
        "\n",
        "Region\n",
        "Keyword\n",
        "Date/Time\n",
        "Search Volume\n",
        "Related Keywords\n",
        "'''"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "8c156658-b4af-4937-c61c-3b19a149ceb3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Research Question: How do search trends related to climate change differ across regions, and what factors influence these differences?\\n\\nData Needed:\\nSearch terms related to climate change (\"global warming,\" \"carbon footprint\").\\nGeographic location (country, city, or region).\\nDate and time of searches.\\nSearch frequency (number of searches for each term).\\nPotentially related keywords and topics.\\n\\nAmount of Data:\\nA dataset of 1000 samples of search trends, with at least 100 samples from 10 different regions would be sufficient for initial analysis.\\n\\nSteps for Collecting Data:\\n\\nChoose a Scraping Method\\nDefine Search Keywords: Select a list of search terms related to climate change, such as \"climate change,\" \"global warming,\" \"renewable energy,\" etc.\\nSelect regions for comparison.\\nCollect Data: Use the pytrends library to collect search trend data for the specified keywords and regions.\\nSet a time frame (monthly or yearly trends) and gather data.\\nSave Data:\\nStore the collected data in a CSV file with the following columns:\\n\\nRegion\\nKeyword\\nDate/Time\\nSearch Volume\\nRelated Keywords\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytrends --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFmfxKk1Ts1-",
        "outputId": "b764baec-867e-4da2-b4bb-ba3b75230dcb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytrends in /usr/local/lib/python3.10/dist-packages (4.9.2)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.32.3)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.1.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pytrends) (4.9.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "from pytrends.request import TrendReq\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Initialize pytrends\n",
        "pytrends = TrendReq(hl='en-US', tz=360)\n",
        "\n",
        "# Define the search terms and regions\n",
        "keywords = [\"climate change\", \"global warming\", \"carbon footprint\", \"renewable energy\"]\n",
        "regions = ['US', 'GB', 'IN', 'DE', 'FR', 'BR', 'CA', 'AU', 'ZA', 'RU']\n",
        "\n",
        "# List to store all dataframes\n",
        "all_data = []\n",
        "\n",
        "# Function to handle requests with a delay\n",
        "def safe_request(pytrends, keyword, region):\n",
        "    success = False\n",
        "    while not success:\n",
        "        try:\n",
        "            pytrends.build_payload([keyword], cat=0, timeframe='today 12-m', geo=region, gprop='')\n",
        "            data = pytrends.interest_over_time()\n",
        "            success = True\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            if isinstance(e, pytrends.exceptions.TooManyRequestsError):\n",
        "                print(\"Hit rate limit. Waiting before retrying...\")\n",
        "                time.sleep(60)  # Wait for 60 seconds before retrying\n",
        "            else:\n",
        "                print(f\"An error occurred: {e}\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "# Collect search data for each region and keyword\n",
        "for region in regions:\n",
        "    for keyword in keywords:\n",
        "        data = safe_request(pytrends, keyword, region)\n",
        "\n",
        "        # Only keep data with actual results (no missing data)\n",
        "        if not data.empty:\n",
        "            data = data.reset_index()\n",
        "            data['Region'] = region\n",
        "            data['Keyword'] = keyword\n",
        "            all_data.append(data)\n",
        "\n",
        "        # Stop once we reach 1000 samples\n",
        "        if sum([len(df) for df in all_data]) >= 1000:\n",
        "            break\n",
        "    if sum([len(df) for df in all_data]) >= 1000:\n",
        "        break\n",
        "\n",
        "# Concatenate all dataframes into one\n",
        "all_data = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "# Save to CSV\n",
        "all_data.to_csv('climate_change_search_trends.csv', index=False)\n",
        "\n",
        "print(f\"Collected {len(all_data)} samples and saved to 'climate_change_search_trends.csv'\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2af2f77-dc4a-460f-bc19-54c7d95462b9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 1007 samples and saved to 'climate_change_search_trends.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e8f5d2d-53d2-4df2-9d65-c949b2a1145e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 10/1000 papers.\n",
            "Retrieved 20/1000 papers.\n",
            "Retrieved 30/1000 papers.\n",
            "Retrieved 40/1000 papers.\n",
            "Retrieved 50/1000 papers.\n",
            "Retrieved 60/1000 papers.\n",
            "Retrieved 70/1000 papers.\n",
            "Retrieved 80/1000 papers.\n",
            "Retrieved 90/1000 papers.\n",
            "Retrieved 100/1000 papers.\n",
            "Retrieved 110/1000 papers.\n",
            "Retrieved 120/1000 papers.\n",
            "Retrieved 130/1000 papers.\n",
            "Retrieved 140/1000 papers.\n",
            "Retrieved 150/1000 papers.\n",
            "Retrieved 160/1000 papers.\n",
            "Retrieved 170/1000 papers.\n",
            "Retrieved 180/1000 papers.\n",
            "Retrieved 190/1000 papers.\n",
            "Retrieved 200/1000 papers.\n",
            "Retrieved 210/1000 papers.\n",
            "Retrieved 220/1000 papers.\n",
            "Retrieved 230/1000 papers.\n",
            "Retrieved 240/1000 papers.\n",
            "Retrieved 250/1000 papers.\n",
            "Retrieved 260/1000 papers.\n",
            "Retrieved 270/1000 papers.\n",
            "Retrieved 280/1000 papers.\n",
            "Retrieved 290/1000 papers.\n",
            "Retrieved 300/1000 papers.\n",
            "Retrieved 310/1000 papers.\n",
            "Retrieved 320/1000 papers.\n",
            "Retrieved 330/1000 papers.\n",
            "Retrieved 340/1000 papers.\n",
            "Retrieved 350/1000 papers.\n",
            "Retrieved 360/1000 papers.\n",
            "Retrieved 370/1000 papers.\n",
            "Retrieved 380/1000 papers.\n",
            "Retrieved 390/1000 papers.\n",
            "Retrieved 400/1000 papers.\n",
            "Retrieved 410/1000 papers.\n",
            "Retrieved 420/1000 papers.\n",
            "Retrieved 430/1000 papers.\n",
            "Failed to retrieve data: Status code 429\n",
            "Data collection complete. Saved to google_scholar_articles_data.csv\n"
          ]
        }
      ],
      "source": [
        "# write your answer here\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def fetch_scholar_data(keyword, num_papers, start_year, end_year):\n",
        "    base_url = \"https://scholar.google.com/scholar\"\n",
        "    collected_data = []\n",
        "    params = {\n",
        "        'q': keyword,\n",
        "        'hl': 'en',\n",
        "        'as_ylo': start_year,\n",
        "        'as_yhi': end_year\n",
        "    }\n",
        "\n",
        "    # Loop to paginate through results\n",
        "    for start in range(0, num_papers, 10):\n",
        "        params['start'] = start\n",
        "        response = requests.get(base_url, params=params)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve data: Status code {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find all result containers\n",
        "        results = soup.find_all('div', class_='gs_ri')\n",
        "        if not results:\n",
        "            print(\"No more results found or error in parsing.\")\n",
        "            break\n",
        "\n",
        "        for result in results:\n",
        "            title_elem = result.find('h3', class_='gs_rt')\n",
        "            title = title_elem.text if title_elem else 'N/A'\n",
        "\n",
        "            # Venue/Journal/Conference\n",
        "            venue_elem = result.find('div', class_='gs_a')\n",
        "            venue = venue_elem.text if venue_elem else 'N/A'\n",
        "\n",
        "            # Abstract\n",
        "            abstract_elem = result.find('div', class_='gs_rs')\n",
        "            abstract = abstract_elem.text if abstract_elem else 'N/A'\n",
        "\n",
        "            # Year\n",
        "            year = 'N/A'\n",
        "            for text in venue.split():\n",
        "                if text.isdigit() and len(text) == 4 and start_year <= int(text) <= end_year:\n",
        "                    year = text\n",
        "                    break\n",
        "\n",
        "            # Authors\n",
        "            authors = venue.split('-')[0].strip()\n",
        "\n",
        "            collected_data.append({\n",
        "                'Title': title,\n",
        "                'Venue': venue,\n",
        "                'Year': year,\n",
        "                'Authors': authors,\n",
        "                'Abstract': abstract\n",
        "            })\n",
        "\n",
        "        # Progress reporting\n",
        "        print(f\"Retrieved {len(collected_data)}/{num_papers} papers.\")\n",
        "\n",
        "        # Delay to avoid hitting rate limits\n",
        "        time.sleep(10)\n",
        "\n",
        "    return collected_data\n",
        "\n",
        "# Collect 1000 papers published between 2014 and 2024 with the keyword \"XYZ\"\n",
        "papers = fetch_scholar_data(keyword=\"XYZ\", num_papers=1000, start_year=2014, end_year=2024)\n",
        "\n",
        "# Check if any data was collected before saving\n",
        "if papers:\n",
        "    # Save to CSV\n",
        "    df = pd.DataFrame(papers)\n",
        "    df.to_csv('google_scholar_articles_data.csv', index=False)\n",
        "    print(\"Data collection complete. Saved to google_scholar_articles_data.csv\")\n",
        "else:\n",
        "    print(\"No data collected. Please check the scraping process and response.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ljGPGsmZJ_F",
        "outputId": "d51b4000-eb4d-45ec-b315-7e4fbc7e4d08"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.7.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting prawcore<3,>=2.1 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update-checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.8.30)\n",
            "Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update-checker, prawcore, praw\n",
            "Successfully installed praw-7.7.1 prawcore-2.4.0 update-checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef7e951f-b48d-4455-f464-3cdc6a9d6e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 400 samples and saved to 'reddit_data.csv'\n"
          ]
        }
      ],
      "source": [
        "# write your answer here\n",
        "import praw\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "# Initialize PRAW with Reddit API credentials\n",
        "reddit = praw.Reddit(\n",
        "    client_id='hCx-0eR7fztbJlBX_yJmBA',\n",
        "    client_secret='5TJxHcY3Qs5LyPmxtYeq0ULFwZ_DWw',\n",
        "    user_agent='dev1'\n",
        ")\n",
        "\n",
        "# Define search parameters\n",
        "keywords = [\"climate change\", \"global warming\", \"carbon footprint\", \"renewable energy\"]\n",
        "subreddits = ['all']\n",
        "limit = 100  # Number of posts to fetch per keyword\n",
        "\n",
        "# List to store data\n",
        "data_list = []\n",
        "\n",
        "# Function to fetch posts for a given keyword\n",
        "def fetch_reddit_data(keyword):\n",
        "    for subreddit in subreddits:\n",
        "        for submission in reddit.subreddit(subreddit).search(keyword, limit=limit):\n",
        "            data = {\n",
        "                'Title': submission.title,\n",
        "                'Author': submission.author.name if submission.author else 'N/A',\n",
        "                'Score': submission.score,\n",
        "                'Created_At': datetime.datetime.fromtimestamp(submission.created_utc),\n",
        "                'URL': submission.url,\n",
        "                'Keyword': keyword\n",
        "            }\n",
        "            data_list.append(data)\n",
        "\n",
        "# Collect data for each keyword\n",
        "for keyword in keywords:\n",
        "    fetch_reddit_data(keyword)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data_list)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('reddit_data.csv', index=False)\n",
        "\n",
        "print(f\"Collected {len(df)} samples and saved to 'reddit_data.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "I appreciate the way the course is taught, with clear explanations and examples that make complex concepts easier to understand. The practical insights into web scraping and data collection have been particularly valuable. However, it would be helpful to see more examples during lectures to solidify understanding. For in-class tasks, additional explanations and ideas on how to approach the questions would be beneficial.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "993d689c-2c68-4712-cf2b-7e54696f8515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nI appreciate the way the course is taught, with clear explanations and examples that make complex concepts easier to understand. The practical insights into web scraping and data collection have been particularly valuable. However, it would be helpful to see more examples during lectures to solidify understanding. For in-class tasks, additional explanations and ideas on how to approach the questions would be beneficial.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}